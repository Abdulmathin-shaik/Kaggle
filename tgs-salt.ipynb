{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10151,"databundleVersionId":59042,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-24T03:40:32.212764Z","iopub.execute_input":"2025-04-24T03:40:32.213125Z","iopub.status.idle":"2025-04-24T03:40:32.562339Z","shell.execute_reply.started":"2025-04-24T03:40:32.213097Z","shell.execute_reply":"2025-04-24T03:40:32.561459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\nimport os\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns  \nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\n\n\n# %%\ntrain_dir = r'D:\\TGS Salt\\train'\n\n\n# %%\nimage_dir = 'train\\images'\nmask_dir = 'train\\masks'\n\n# %%\nclass SaltDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, image_transform=None, mask_transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.image_transform = image_transform\n        self.mask_transform = mask_transform\n        self.images = os.listdir(image_dir)\n        self.masks = os.listdir(mask_dir)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.images[idx])\n        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n        \n        image = Image.open(img_path).convert('RGB')\n        mask = Image.open(mask_path).convert('L')\n\n        if self.image_transform:\n            image = self.image_transform(image)\n        if self.mask_transform:\n            mask = self.mask_transform(mask)\n\n        return image, mask\n\n# %%\nimage_transform = T.Compose([\n    T.Resize((128, 128)),\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nmask_transform = T.Compose([\n    T.Resize((128, 128)),\n    T.ToTensor(),  # will keep mask as [1, H, W]\n])\n\n\n# %%\ntrain_dataset = SaltDataset(image_dir,mask_dir,image_transform, mask_transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# %%\ntrain_dataset[0][0].shape\n\n# %%\nimport matplotlib.pyplot as plt\n\n# Pick one sample (e.g., the first one)\nimage, mask = train_dataset[20]\n\n# Convert tensors to numpy arrays for visualization\n# For image: [C, H, W] -> [H, W, C]\nimg_np = image.permute(1, 2, 0).cpu().numpy()\n# For mask: [1, H, W] -> [H, W]\nmask_np = mask.squeeze().cpu().numpy()\n\n# Undo normalization for visualization if needed\n# (If you used normalization in image_transform)\nimg_np = img_np * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\nimg_np = img_np.clip(0, 1)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.title(\"Image\")\nplt.imshow(img_np)\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.title(\"Mask\")\nplt.imshow(mask_np, cmap='gray')\nplt.axis('off')\n\nplt.show()\n\n# %%\nclass DoubleConv(nn.Module):\n    def __init__(self,in_ch,out_ch):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_ch,out_ch,3,padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch,out_ch,3,padding=1),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self,x):\n        x = self.double_conv(x)\n        return x\n\n\nclass Unet(nn.Module):\n    def __init__(self):\n        super(Unet, self).__init__()\n        self.enc1 = DoubleConv(3,64)\n        self.pool1 = nn.MaxPool2d(2)\n        self.enc2 = DoubleConv(64,128)\n        self.pool2 = nn.MaxPool2d(2)\n        self.enc3 = DoubleConv(128,256)\n        self.pool3 = nn.MaxPool2d(2)\n        self.enc4 = DoubleConv(256,512)\n        self.pool4 = nn.MaxPool2d(2)\n        \n        self.bottleneck = DoubleConv(512,1024)\n\n        self.up4 = nn.ConvTranspose2d(1024,512,2,stride=2)\n        self.dec4 = DoubleConv(1024,512)\n        self.up3 = nn.ConvTranspose2d(512,256,2,stride=2)\n        self.dec3 = DoubleConv(512,256)\n        self.up2 = nn.ConvTranspose2d(256,128,2,stride=2)\n        self.dec2 = DoubleConv(256,128)\n        self.up1 = nn.ConvTranspose2d(128,64,2,stride=2)\n        self.dec1 = DoubleConv(128,64)\n\n        self.final = nn.Conv2d(64,1,1)\n        \n\n    def forward(self,x):\n        e1 = self.enc1(x)\n        p1 = self.pool1(e1)\n        e2 = self.enc2(p1)\n        p2 = self.pool2(e2)\n        e3 = self.enc3(p2)\n        p3 = self.pool3(e3)\n        e4 = self.enc4(p3)\n        p4 = self.pool4(e4)\n\n        b = self.bottleneck(p4)\n\n        #decoder\n        d4 = self.up4(b)\n        d4 = torch.cat([d4,e4],dim=1)\n        d4 = self.dec4(d4)\n        d3 = self.up3(d4)\n        d3 = torch.cat([d3,e3],dim=1)\n        d3 = self.dec3(d3)\n        d2 = self.up2(d3)\n        d2 = torch.cat([d2,e2],dim=1)\n        d2 = self.dec2(d2)\n        d1 = self.up1(d2)\n        d1 = torch.cat([d1,e1],dim=1)\n        d1 = self.dec1(d1)\n\n        out = self.final(d1)\n\n        return out\n\n\n# %%\nmodel = Unet()\n\n# %%\nx = torch.rand(1,3,128,128)\n\n# %%\nmodel(x)\n\n# %%\nimport matplotlib.pyplot as plt\n\n# Take a sample from your dataset (e.g., the first one)\nsample_img, _ = train_dataset[10]  # (image, mask)\ninput_img = sample_img.unsqueeze(0)  # Add batch dimension\n\n# Put model in eval mode and get output\nmodel.eval()\nwith torch.no_grad():\n    output = model(input_img)\n    output = torch.sigmoid(output)\n    output_mask = output.squeeze().cpu().numpy()\n\n# Show input and output\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.title(\"Input Image\")\nplt.imshow(sample_img.permute(1,2,0).cpu().numpy())\nplt.axis('off')\n\nplt.subplot(1,2,2)\nplt.title(\"Predicted Mask\")\nplt.imshow(output_mask, cmap='gray')\nplt.axis('off')\nplt.show()\n\n# %%\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns  \nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T03:42:00.567129Z","iopub.execute_input":"2025-04-24T03:42:00.567903Z","iopub.status.idle":"2025-04-24T03:42:09.852049Z","shell.execute_reply.started":"2025-04-24T03:42:00.567870Z","shell.execute_reply":"2025-04-24T03:42:09.851367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !unzip /kaggle/input/tgs-salt-identification-challenge/competition_data.zip","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!unzip /kaggle/input/tgs-salt-identification-challenge/train.zip","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(os.listdir('/kaggle/working/masks'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T03:40:53.457562Z","iopub.execute_input":"2025-04-24T03:40:53.457888Z","iopub.status.idle":"2025-04-24T03:40:53.468167Z","shell.execute_reply.started":"2025-04-24T03:40:53.457862Z","shell.execute_reply":"2025-04-24T03:40:53.467370Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_dir = '/kaggle/working/images'\nmask_dir = '/kaggle/working/masks'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T03:40:54.893816Z","iopub.execute_input":"2025-04-24T03:40:54.894104Z","iopub.status.idle":"2025-04-24T03:40:54.898637Z","shell.execute_reply.started":"2025-04-24T03:40:54.894083Z","shell.execute_reply":"2025-04-24T03:40:54.897657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for k in os.listdir(image_dir):\n    img = os.path.join(image_dir,k)\n    plt.imshow(cv2.imread(img))\n    print(k)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T03:41:33.542953Z","iopub.execute_input":"2025-04-24T03:41:33.543688Z","iopub.status.idle":"2025-04-24T03:41:33.806059Z","shell.execute_reply.started":"2025-04-24T03:41:33.543648Z","shell.execute_reply":"2025-04-24T03:41:33.805290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for k in os.listdir(mask_dir):\n    img = os.path.join(mask_dir,k)\n    plt.imshow(cv2.imread(img))\n    print(k)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T03:41:37.902685Z","iopub.execute_input":"2025-04-24T03:41:37.902995Z","iopub.status.idle":"2025-04-24T03:41:38.142201Z","shell.execute_reply.started":"2025-04-24T03:41:37.902972Z","shell.execute_reply":"2025-04-24T03:41:38.141182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SaltDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, image_transform=None, mask_transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.image_transform = image_transform\n        self.mask_transform = mask_transform\n        self.images = os.listdir(image_dir)\n        self.masks = os.listdir(mask_dir)\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.images[idx])\n        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n        \n        # Load image and mask as grayscale (1 channel)\n        image = Image.open(img_path).convert('L')  # Grayscale image\n        mask = Image.open(mask_path).convert('L')  # Grayscale mask\n\n        # Apply transforms if provided\n        if self.image_transform:\n            image = self.image_transform(image)\n        if self.mask_transform:\n            mask = self.mask_transform(mask)\n\n        # Convert to tensors if still a PIL Image\n        if isinstance(image, Image.Image):\n            image = np.array(image, dtype=np.float32) / 255.0\n            image = torch.from_numpy(image).unsqueeze(0)  # [1, H, W]\n        if isinstance(mask, Image.Image):\n            mask = np.array(mask, dtype=np.float32) / 255.0\n            mask = torch.from_numpy(mask).unsqueeze(0)    # [1, H, W]\n\n        # If mask has 3 channels after transform, convert to 1 channel\n        if isinstance(mask, torch.Tensor) and mask.shape[0] == 3:\n            mask = mask.mean(dim=0, keepdim=True)  # Average across channels to get [1, H, W]\n\n        # If image has 3 channels after transform, convert to 1 channel (just in case)\n        if isinstance(image, torch.Tensor) and image.shape[0] == 3:\n            image = image.mean(dim=0, keepdim=True)  # Average across channels to get [1, H, W]\n\n        return image, mask\n\n# %%\nimage_transform = T.Compose([\n    T.Resize((128, 128)),\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nmask_transform = T.Compose([\n    T.Resize((128, 128)),\n    T.ToTensor(),  # will keep mask as [1, H, W]\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:10:18.511541Z","iopub.execute_input":"2025-04-24T04:10:18.511856Z","iopub.status.idle":"2025-04-24T04:10:18.522096Z","shell.execute_reply.started":"2025-04-24T04:10:18.511834Z","shell.execute_reply":"2025-04-24T04:10:18.521293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\ntrain_dataset = SaltDataset(image_dir,mask_dir,image_transform, mask_transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:10:19.829570Z","iopub.execute_input":"2025-04-24T04:10:19.829874Z","iopub.status.idle":"2025-04-24T04:10:19.840426Z","shell.execute_reply.started":"2025-04-24T04:10:19.829854Z","shell.execute_reply":"2025-04-24T04:10:19.839480Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[100][0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:10:23.117010Z","iopub.execute_input":"2025-04-24T04:10:23.117696Z","iopub.status.idle":"2025-04-24T04:10:23.145736Z","shell.execute_reply.started":"2025-04-24T04:10:23.117669Z","shell.execute_reply":"2025-04-24T04:10:23.144478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        # Helper function to create a convolutional block\n        def conv_block(in_channels, out_channels):\n            return nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n                nn.ReLU(inplace=True)\n            )\n\n        # Contracting Path (Encoder)\n        self.enc1 = conv_block(1, 32)   # Input: 128x128x1, Output: 128x128x32\n        self.pool1 = nn.MaxPool2d(2)   # Output: 64x64x32\n        self.enc2 = conv_block(32, 64)  # Output: 64x64x64\n        self.pool2 = nn.MaxPool2d(2)   # Output: 32x32x64\n        self.enc3 = conv_block(64, 128) # Output: 32x32x128\n        self.pool3 = nn.MaxPool2d(2)   # Output: 16x16x128\n\n        # Bottleneck\n        self.bottleneck = conv_block(128, 256)  # Output: 16x16x256\n\n        # Expanding Path (Decoder)\n        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)  # Output: 32x32x128\n        self.dec3 = conv_block(256, 128)  # Concat 128 (up) + 128 (skip) = 256, Output: 32x32x128\n        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)   # Output: 64x64x64\n        self.dec2 = conv_block(128, 64)   # Concat 64 (up) + 64 (skip) = 128, Output: 64x64x64\n        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)    # Output: 128x128x32\n        self.dec1 = conv_block(64, 32)    # Concat 32 (up) + 32 (skip) = 64, Output: 128x128x32\n\n        # Output Layer\n        self.final_conv = nn.Conv2d(32, 1, kernel_size=1)  # Output: 128x128x1\n\n    def forward(self, x):\n        # Encoder\n        enc1 = self.enc1(x)       # 128x128x32\n        pool1 = self.pool1(enc1)  # 64x64x32\n        enc2 = self.enc2(pool1)   # 64x64x64\n        pool2 = self.pool2(enc2)  # 32x32x64\n        enc3 = self.enc3(pool2)   # 32x32x128\n        pool3 = self.pool3(enc3)  # 16x16x128\n\n        # Bottleneck\n        bottleneck = self.bottleneck(pool3)  # 16x16x256\n\n        # Decoder with Skip Connections\n        up3 = self.upconv3(bottleneck)        # 32x32x128\n        merge3 = torch.cat([up3, enc3], dim=1)  # 32x32x256 (128 + 128)\n        dec3 = self.dec3(merge3)              # 32x32x128\n        up2 = self.upconv2(dec3)              # 64x64x64\n        merge2 = torch.cat([up2, enc2], dim=1)  # 64x64x128 (64 + 64)\n        dec2 = self.dec2(merge2)              # 64x64x64\n        up1 = self.upconv1(dec2)              # 128x128x32\n        merge1 = torch.cat([up1, enc1], dim=1)  # 128x128x64 (32 + 32)\n        dec1 = self.dec1(merge1)              # 128x128x32\n\n        # Output\n        out = self.final_conv(dec1)           # 128x128x1\n        return torch.sigmoid(out)             # Probabilities [0, 1]\n\n# Test the model\nif __name__ == \"__main__\":\n    model = UNet()\n    x = torch.randn(1, 1, 128, 128)  # Batch of 1, 1 channel, 128x128\n    y = model(x)\n    print(f\"Input shape: {x.shape}\")\n    print(f\"Output shape: {y.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T04:10:24.771206Z","iopub.execute_input":"2025-04-24T04:10:24.772018Z","iopub.status.idle":"2025-04-24T04:10:24.880106Z","shell.execute_reply.started":"2025-04-24T04:10:24.771993Z","shell.execute_reply":"2025-04-24T04:10:24.879210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming SaltDataset is defined as before\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\n\n\n# Model, Loss, and Optimizer\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = UNet().to(device)  # Or use the basic UNet()\ncriterion = nn.BCELoss()  # Binary Cross-Entropy for binary segmentation\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training Loop (Basic)\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for images, masks in train_loader:\n        images, masks = images.to(device), masks.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T03:50:35.366276Z","iopub.execute_input":"2025-04-24T03:50:35.366623Z","iopub.status.idle":"2025-04-24T03:50:35.553052Z","shell.execute_reply.started":"2025-04-24T03:50:35.366600Z","shell.execute_reply":"2025-04-24T03:50:35.551796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}