{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPI5HG3Fs+3w9l0f4tt188/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdulmathin-shaik/Kaggle/blob/main/FMNIST_100th_time_because_you_suck!.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jnbhAjm457Kn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "mgQTGM4m6LQg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform_func = transforms.Compose([ToTensor()])\n"
      ],
      "metadata": {
        "id": "5OxsJeCM6Mwj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_fmnist = datasets.FashionMNIST(root='data',train=True,download=True,transform=ToTensor())\n",
        "test_fmnist = datasets.FashionMNIST(root='data',train=False,transform=ToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umWtrVxR6NtP",
        "outputId": "4fbefe62-2c91-45e1-cf90-6a80be868ff7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 13.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 201kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.72MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 23.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_fmnist,batch_size=64,shuffle=True)\n",
        "test_dataloader = DataLoader(test_fmnist,batch_size=64)"
      ],
      "metadata": {
        "id": "1Fd3NBhC6Qcu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class simple_model(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.stack = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32,64,3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            # nn.Dropout2d(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64,64,3),\n",
        "            # nn.Dropout2d(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64,128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128,128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128,10)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        # x = self.flatten(x)\n",
        "        x = self.stack(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WJSBnz3W6S4a"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = simple_model().to(device)"
      ],
      "metadata": {
        "id": "O06WF1_D6ZPM"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.randn(64,1,28,28).to(device)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mS-Od-9mEOOV",
        "outputId": "8798dfaf-682d-4dd2-bef4-600b4bf19e99"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-04)"
      ],
      "metadata": {
        "id": "OC-0MQUq6a1W"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses,test_losses = [],[]\n",
        "train_accs,test_accs =[],[]\n",
        "for epoch in range(epochs):\n",
        "    train_loss=0\n",
        "    for batch,(x,y) in enumerate(train_dataloader):\n",
        "        model.train()\n",
        "        x,y = x.to(device),y.to(device)\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred.squeeze(0),y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        train_loss += loss.item()\n",
        "    epoch_train_loss = train_loss/len(train_dataloader)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    print(f'Training loss for epoch {epoch}: {epoch_train_loss}')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss,test_acc=0,0\n",
        "        for batch,(x,y) in enumerate(test_dataloader):\n",
        "            model.eval()\n",
        "            x,y = x.to(device),y.to(device)\n",
        "            test_pred = model(x)\n",
        "            loss = loss_fn(test_pred,y)\n",
        "            test_loss += loss.item()\n",
        "            test_acc += ((y==test_pred.argmax(dim=1)).sum().item())/len(y)\n",
        "        epoch_test_loss = train_loss/len(test_dataloader)\n",
        "        epoch_acc = test_acc/len(test_dataloader)\n",
        "        test_losses.append(epoch_test_loss)\n",
        "        test_accs.append(epoch_acc)\n",
        "\n",
        "        print(f'Training loss for epoch {epoch}: {epoch_test_loss} and test accuracy is {epoch_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubslLzaU6duZ",
        "outputId": "0fc8948d-0ad8-455f-986b-12996a7bdae3"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss for epoch 0: 0.8195766402460111\n",
            "Training loss for epoch 0: 4.896578907966614 and test accuracy is 0.8003582802547771\n",
            "Training loss for epoch 1: 0.47654389609088266\n",
            "Training loss for epoch 1: 2.847122130785019 and test accuracy is 0.8269307324840764\n",
            "Training loss for epoch 2: 0.41116491698824775\n",
            "Training loss for epoch 2: 2.456513962643162 and test accuracy is 0.8429538216560509\n",
            "Training loss for epoch 3: 0.3753816590253224\n",
            "Training loss for epoch 3: 2.2427260902277224 and test accuracy is 0.8568869426751592\n",
            "Training loss for epoch 4: 0.3477785639099475\n",
            "Training loss for epoch 4: 2.077810783105291 and test accuracy is 0.8597730891719745\n",
            "Training loss for epoch 5: 0.329400286396175\n",
            "Training loss for epoch 5: 1.9680093543924344 and test accuracy is 0.8664410828025477\n",
            "Training loss for epoch 6: 0.31236071805201615\n",
            "Training loss for epoch 6: 1.8662060734572683 and test accuracy is 0.869327229299363\n",
            "Training loss for epoch 7: 0.2989325499706177\n",
            "Training loss for epoch 7: 1.7859791839008878 and test accuracy is 0.8730095541401274\n",
            "Training loss for epoch 8: 0.28469881466202646\n",
            "Training loss for epoch 8: 1.7009394149871389 and test accuracy is 0.8756966560509554\n",
            "Training loss for epoch 9: 0.27485876687681243\n",
            "Training loss for epoch 9: 1.642149830130255 and test accuracy is 0.8702229299363057\n",
            "Training loss for epoch 10: 0.26145929067945683\n",
            "Training loss for epoch 10: 1.5620943608747166 and test accuracy is 0.87609474522293\n",
            "Training loss for epoch 11: 0.2525445962829122\n",
            "Training loss for epoch 11: 1.5088333204673354 and test accuracy is 0.8765923566878981\n",
            "Training loss for epoch 12: 0.24357277003210237\n",
            "Training loss for epoch 12: 1.4552309445230065 and test accuracy is 0.8765923566878981\n",
            "Training loss for epoch 13: 0.2354569489807526\n",
            "Training loss for epoch 13: 1.4067427907257704 and test accuracy is 0.8777866242038217\n",
            "Training loss for epoch 14: 0.2258506200429219\n",
            "Training loss for epoch 14: 1.349349564332871 and test accuracy is 0.8836584394904459\n",
            "Training loss for epoch 15: 0.2181118875503667\n",
            "Training loss for epoch 15: 1.3031143345365859 and test accuracy is 0.8801751592356688\n",
            "Training loss for epoch 16: 0.21021274713946303\n",
            "Training loss for epoch 16: 1.255920744056155 and test accuracy is 0.8859474522292994\n",
            "Training loss for epoch 17: 0.20094800489480052\n",
            "Training loss for epoch 17: 1.200568334976579 and test accuracy is 0.8824641719745223\n",
            "Training loss for epoch 18: 0.1969944902741388\n",
            "Training loss for epoch 18: 1.1769479737397592 and test accuracy is 0.8816679936305732\n",
            "Training loss for epoch 19: 0.18780408229337317\n",
            "Training loss for epoch 19: 1.1220396763769684 and test accuracy is 0.8784832802547771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses,label='train_loss')\n",
        "plt.plot(test_losses,label='test_loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Hg7xW1e2IEV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(train_accs,label='train_acc')\n",
        "plt.plot(test_accs,label='test_acc')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "BSclmcQzILZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y=next(iter(test_dataloader))"
      ],
      "metadata": {
        "id": "ps_IkMVv6ho7"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_preds = model(x.to(device))"
      ],
      "metadata": {
        "id": "JPrZT8KJFANQ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_preds.argmax(dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lw_pkytFJBu",
        "outputId": "4145340e-426e-45af-cd23-61ae43a26ca8"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 5, 3, 4, 1, 2, 4, 8, 0, 2, 5, 7, 5,\n",
              "        1, 6, 6, 0, 9, 4, 8, 8, 1, 3, 8, 0, 7, 5, 7, 9, 6, 1, 3, 9, 4, 7, 2, 1,\n",
              "        4, 6, 4, 6, 5, 6, 2, 2, 8, 4, 8, 0, 7, 7, 8, 5], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVI4dElTFQmu",
        "outputId": "99743365-5fd7-4d30-f45b-b1cc8f1cd50a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 7, 3, 4, 1, 2, 4, 8, 0, 2, 5, 7, 9,\n",
              "        1, 4, 6, 0, 9, 3, 8, 8, 3, 3, 8, 0, 7, 5, 7, 9, 6, 1, 3, 7, 6, 7, 2, 1,\n",
              "        2, 2, 4, 4, 5, 8, 2, 2, 8, 4, 8, 0, 7, 7, 8, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model1_accurcy =\n",
        "(final_preds.argmax(dim=1)==y.to(device)).sum().item()/len(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrqQYP1SFg_X",
        "outputId": "0bbae740-f539-4d8f-a625-40125ac3d202"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.828125"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "zt6ViKCcFlAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm"
      ],
      "metadata": {
        "id": "RHHH9oyHJVxY"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tl_model = timm.create_model('resnet50',pretrained=True)\n",
        "tl_model = torchvision.models.resnet50(pretrained=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cctTsU0SJWwi",
        "outputId": "34da0531-02c1-4a17-e367-c704e8da4797"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 187MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tl_model.fc = nn.Linear(2048,10)"
      ],
      "metadata": {
        "id": "m4Tw5CfnJZcQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tl_model = tl_model.to(device)"
      ],
      "metadata": {
        "id": "hHbdXGH8KEnB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tl_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcXtr_OaWg5b",
        "outputId": "4090e8f3-95ff-402e-fec4-6ce4b6e9f91b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB\n",
        "    transforms.Resize((224, 224)),  # Resize for ResNet\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)  # Normalize for 3 channels\n",
        "])\n"
      ],
      "metadata": {
        "id": "9dMEjZWHQJWo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "tl_optimizer = torch.optim.Adam(tl_model.parameters(),lr=1e-04)"
      ],
      "metadata": {
        "id": "cBEo1YvxKOCW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses,test_losses = [],[]\n",
        "train_accs,test_accs =[],[]\n",
        "for epoch in range(epochs):\n",
        "    train_loss=0\n",
        "    for batch,(x,y) in enumerate(train_dataloader):\n",
        "        tl_model.train()\n",
        "        x,y = x.to(device),y.to(device)\n",
        "        pred = tl_model(x.repeat(1,3,1,1))\n",
        "        loss = loss_fn(pred.squeeze(0),y)\n",
        "        loss.backward()\n",
        "        tl_optimizer.step()\n",
        "        tl_optimizer.zero_grad()\n",
        "        train_loss += loss.item()\n",
        "    epoch_train_loss = train_loss/len(train_dataloader)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    print(f'Training loss for epoch {epoch}: {epoch_train_loss}')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss,test_acc=0,0\n",
        "        for batch,(x,y) in enumerate(test_dataloader):\n",
        "            tl_model.eval()\n",
        "            x,y = x.to(device),y.to(device)\n",
        "            test_pred = tl_model(x.repeat(1,3,1,1))\n",
        "            loss = loss_fn(test_pred,y)\n",
        "            test_loss += loss.item()\n",
        "            test_acc += ((y==test_pred.argmax(dim=1)).sum().item())/len(y)\n",
        "        epoch_test_loss = train_loss/len(test_dataloader)\n",
        "        epoch_acc = test_acc/len(test_dataloader)\n",
        "        test_losses.append(epoch_test_loss)\n",
        "        test_accs.append(epoch_acc)\n",
        "\n",
        "        print(f'Training loss for epoch {epoch}: {epoch_test_loss} and test accuracy is {epoch_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIne_2sdKTQz",
        "outputId": "74d4af5a-e78d-4657-89ea-a2b28d8c255d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss for epoch 0: 0.3691533633799695\n",
            "Training loss for epoch 0: 2.2055149990472063 and test accuracy is 0.908140923566879\n",
            "Training loss for epoch 1: 0.22027901381746665\n",
            "Training loss for epoch 1: 1.3160618787311065 and test accuracy is 0.9136146496815286\n",
            "Training loss for epoch 2: 0.17880467687254903\n",
            "Training loss for epoch 2: 1.0682725280665668 and test accuracy is 0.9214769108280255\n",
            "Training loss for epoch 3: 0.14223831296284825\n",
            "Training loss for epoch 3: 0.8498059717143417 and test accuracy is 0.9147093949044586\n",
            "Training loss for epoch 4: 0.12044661516236312\n",
            "Training loss for epoch 4: 0.7196109874031631 and test accuracy is 0.9244625796178344\n",
            "Training loss for epoch 5: 0.10075404895330543\n",
            "Training loss for epoch 5: 0.6019573115808948 and test accuracy is 0.9240644904458599\n",
            "Training loss for epoch 6: 0.08008724728575362\n",
            "Training loss for epoch 6: 0.47848304429322863 and test accuracy is 0.9196855095541401\n",
            "Training loss for epoch 7: 0.07004276030500338\n",
            "Training loss for epoch 7: 0.418472032905052 and test accuracy is 0.9263535031847133\n",
            "Training loss for epoch 8: 0.055036339478920705\n",
            "Training loss for epoch 8: 0.32881583714157725 and test accuracy is 0.927547770700637\n",
            "Training loss for epoch 9: 0.045799734105275616\n",
            "Training loss for epoch 9: 0.27363153242514987 and test accuracy is 0.9247611464968153\n",
            "Training loss for epoch 10: 0.04044267570953379\n",
            "Training loss for epoch 10: 0.24162566761492163 and test accuracy is 0.9249601910828026\n",
            "Training loss for epoch 11: 0.037291939977853336\n",
            "Training loss for epoch 11: 0.22280152674666517 and test accuracy is 0.9287420382165605\n",
            "Training loss for epoch 12: 0.031955540367054135\n",
            "Training loss for epoch 12: 0.19091908830762278 and test accuracy is 0.9252587579617835\n",
            "Training loss for epoch 13: 0.03128858978842953\n",
            "Training loss for epoch 13: 0.1869343772073051 and test accuracy is 0.9309315286624203\n",
            "Training loss for epoch 14: 0.023919487488890456\n",
            "Training loss for epoch 14: 0.14290751123935827 and test accuracy is 0.9246616242038217\n",
            "Training loss for epoch 15: 0.030601813429298855\n",
            "Training loss for epoch 15: 0.1828312165393779 and test accuracy is 0.9289410828025477\n",
            "Training loss for epoch 16: 0.02534587448320117\n",
            "Training loss for epoch 16: 0.15142949213530382 and test accuracy is 0.9286425159235668\n",
            "Training loss for epoch 17: 0.02269228510891841\n",
            "Training loss for epoch 17: 0.13557556326220044 and test accuracy is 0.9287420382165605\n",
            "Training loss for epoch 18: 0.02146339756872893\n",
            "Training loss for epoch 18: 0.1282335472577563 and test accuracy is 0.9257563694267515\n",
            "Training loss for epoch 19: 0.01996789515755181\n",
            "Training loss for epoch 19: 0.11929863476295285 and test accuracy is 0.9281449044585988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alex_net = torchvision.models.vgg16(pretrained=True).to(device)"
      ],
      "metadata": {
        "id": "wLwKMmo0KckM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q-5ZcqoTWQ5_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}