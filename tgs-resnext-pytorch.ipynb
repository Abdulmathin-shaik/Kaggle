{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10151,"databundleVersionId":59042,"sourceType":"competition"},{"sourceId":373444,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":308924,"modelId":329331}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:06:36.062153Z","iopub.execute_input":"2025-05-09T03:06:36.062718Z","iopub.status.idle":"2025-05-09T03:06:36.393222Z","shell.execute_reply.started":"2025-05-09T03:06:36.062697Z","shell.execute_reply":"2025-05-09T03:06:36.392502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nimport torch\nimport seaborn as sns\nimport albumentations as A\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader,Dataset\nfrom PIL import Image\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:06:40.000672Z","iopub.execute_input":"2025-05-09T03:06:40.001081Z","iopub.status.idle":"2025-05-09T03:06:44.372667Z","shell.execute_reply.started":"2025-05-09T03:06:40.001052Z","shell.execute_reply":"2025-05-09T03:06:44.372078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!unzip -q /kaggle/input/tgs-salt-identification-challenge/train.zip -d train_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:58:58.942409Z","iopub.execute_input":"2025-05-09T02:58:58.942987Z","iopub.status.idle":"2025-05-09T02:59:00.191940Z","shell.execute_reply.started":"2025-05-09T02:58:58.942964Z","shell.execute_reply":"2025-05-09T02:59:00.190850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_dir = '/kaggle/working/train_data/images'\nmask_dir = '/kaggle/working/train_data/masks'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:06:44.373625Z","iopub.execute_input":"2025-05-09T03:06:44.374096Z","iopub.status.idle":"2025-05-09T03:06:44.377832Z","shell.execute_reply.started":"2025-05-09T03:06:44.374066Z","shell.execute_reply":"2025-05-09T03:06:44.377078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(os.listdir(mask_dir))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:06:46.284810Z","iopub.execute_input":"2025-05-09T03:06:46.285135Z","iopub.status.idle":"2025-05-09T03:06:46.293357Z","shell.execute_reply.started":"2025-05-09T03:06:46.285112Z","shell.execute_reply":"2025-05-09T03:06:46.292710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Don't need any manipulation as image and mask names are exactly same\nfilenames = os.listdir(image_dir)\n# for k in os.listdir(image_dir):\n#     print(k)\n#     break,\n\ntrain_files,valid_files = train_test_split(filenames,test_size=0.2,random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:06:48.328919Z","iopub.execute_input":"2025-05-09T03:06:48.329500Z","iopub.status.idle":"2025-05-09T03:06:48.336831Z","shell.execute_reply.started":"2025-05-09T03:06:48.329458Z","shell.execute_reply":"2025-05-09T03:06:48.335997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = Image.open(os.path.join(mask_dir,filenames[10])).convert('L')\nimage_np = np.array(image)\nimage_np.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:06:49.754300Z","iopub.execute_input":"2025-05-09T03:06:49.754626Z","iopub.status.idle":"2025-05-09T03:06:49.767004Z","shell.execute_reply.started":"2025-05-09T03:06:49.754604Z","shell.execute_reply":"2025-05-09T03:06:49.766357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Create a dataset \nclass Saltdataset(Dataset):\n    def __init__(self,image_dir,mask_dir,filenames,transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.filenames = filenames\n        self.transform = transform \n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self,idx):\n        image_name = self.filenames[idx]\n        image_path = os.path.join(self.image_dir,image_name)\n        mask_path = os.path.join(self.mask_dir,image_name)\n        image = Image.open(image_path).convert(\"RGB\")\n        mask = Image.open(mask_path).convert(\"L\")\n        #Need to convert to Numpy arrays as Albumentations library needs it\n        image = np.array(image)\n        mask = np.array(mask)/255.0\n\n        \n        \n        if self.transform:\n            augmented = self.transform(image = image,mask = mask)\n            image = augmented['image']\n            mask = augmented['mask']\n            mask = mask.unsqueeze(0)\n\n        # Dont need this as you're using Albumentations\n        # image = torch.tensor(image,dtype=torch.float32).permute(2,0,1)\n        # mask = torch.tensor(mask,dtype=torch.float32).unsqueeze(0)\n\n        return image,mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:06:51.845934Z","iopub.execute_input":"2025-05-09T03:06:51.846440Z","iopub.status.idle":"2025-05-09T03:06:51.852671Z","shell.execute_reply.started":"2025-05-09T03:06:51.846414Z","shell.execute_reply":"2025-05-09T03:06:51.851872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# transform_func = transforms.Compose([transforms.ToTensor()])\n#Main difference between albumentations and torchvision transforms is use of ToTensorV2\ntrain_transform = A.Compose([\n    A.Resize(128,128),\n    A.HorizontalFlip(p=0.5),\n    A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),\n    A.ToTensorV2()\n])\n\nvalid_transform = A.Compose([\n    A.Resize(128,128),\n    # A.HorizontalFlip(p=0.5),\n    A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),\n    A.ToTensorV2()\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:06:54.514498Z","iopub.execute_input":"2025-05-09T03:06:54.514758Z","iopub.status.idle":"2025-05-09T03:06:54.523676Z","shell.execute_reply.started":"2025-05-09T03:06:54.514741Z","shell.execute_reply":"2025-05-09T03:06:54.523083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = Saltdataset(image_dir,mask_dir,train_files,transform=train_transform)\nvalid_dataset = Saltdataset(image_dir,mask_dir,valid_files,transform=valid_transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:06:56.589649Z","iopub.execute_input":"2025-05-09T03:06:56.589933Z","iopub.status.idle":"2025-05-09T03:06:56.594542Z","shell.execute_reply.started":"2025-05-09T03:06:56.589911Z","shell.execute_reply":"2025-05-09T03:06:56.593775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset[3][1].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:06:58.004114Z","iopub.execute_input":"2025-05-09T03:06:58.004398Z","iopub.status.idle":"2025-05-09T03:06:58.011601Z","shell.execute_reply.started":"2025-05-09T03:06:58.004377Z","shell.execute_reply":"2025-05-09T03:06:58.010926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset,batch_size=16,shuffle=True)\nvalid_loader = DataLoader(valid_dataset,batch_size=16,shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:06:58.291768Z","iopub.execute_input":"2025-05-09T03:06:58.292090Z","iopub.status.idle":"2025-05-09T03:06:58.296192Z","shell.execute_reply.started":"2025-05-09T03:06:58.292066Z","shell.execute_reply":"2025-05-09T03:06:58.295419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Doubleconv(nn.Module):\n    def __init__(self,in_ch,out_ch):\n        super().__init__()\n        self.doubleconv = nn.Sequential(\n            nn.Conv2d(in_ch,out_ch,3,padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch,out_ch,3,padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            \n        )\n\n    def forward(self,x):\n        return self.doubleconv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:07:13.629988Z","iopub.execute_input":"2025-05-09T03:07:13.630306Z","iopub.status.idle":"2025-05-09T03:07:13.636407Z","shell.execute_reply.started":"2025-05-09T03:07:13.630284Z","shell.execute_reply":"2025-05-09T03:07:13.635488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nclass Unet(nn.Module):\n    def __init__(self):\n        super(Unet,self).__init__()\n        #encoder\n        self.enc1 = Doubleconv(3,64)\n        self.max1 = nn.MaxPool2d(2)\n        self.enc2 = Doubleconv(64,128)\n        self.max2 = nn.MaxPool2d(2)\n        self.enc3 = Doubleconv(128,256)\n        self.max3 = nn.MaxPool2d(2)\n        self.enc4 = Doubleconv(256,512)\n        self.max4 = nn.MaxPool2d(2)\n\n        #bottleneck\n        self.bottleneck = Doubleconv(512,1024)\n\n        #decoder\n        self.up1 = nn.ConvTranspose2d(1024,512,2,stride=2) #upsample\n        self.dec1 = Doubleconv(1024,512)\n        self.up2 = nn.ConvTranspose2d(512,256,2,stride=2) #upsample\n        self.dec2 = Doubleconv(512,256)\n        self.up3 = nn.ConvTranspose2d(256,128,2,stride=2) #upsample\n        self.dec3 = Doubleconv(256,128)\n        self.up4 = nn.ConvTranspose2d(128,64,2,stride=2) #upsample\n        self.dec4 = Doubleconv(128,64)\n\n        self.final = nn.Conv2d(64,1,1)\n\n    def forward(self,x):\n        x1 = self.enc1(x)\n        x2 = self.enc2(self.max1(x1))\n        x3 = self.enc3(self.max2(x2))\n        x4 = self.enc4(self.max3(x3))\n\n        #bottleneck\n        x5 = self.bottleneck(self.max4(x4))\n\n        #decoder\n        d1 = self.up1(x5)\n        d1 = torch.cat([d1,x4],dim=1)\n        d1 = self.dec1(d1)\n\n        d2 = self.up2(d1)\n        d2 = torch.cat([d2,x3],dim=1)\n        d2 = self.dec2(d2)\n\n        d3 = self.up3(d2)\n        d3 = torch.cat([d3,x2],dim=1)\n        d3 = self.dec3(d3)\n\n        d4 = self.up4(d3)\n        d4 = torch.cat([d4,x1],dim=1)\n        d4 = self.dec4(d4)\n\n        output = self.final(d4)\n\n        return output\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:59:23.398941Z","iopub.execute_input":"2025-05-09T02:59:23.399549Z","iopub.status.idle":"2025-05-09T02:59:23.410607Z","shell.execute_reply.started":"2025-05-09T02:59:23.399516Z","shell.execute_reply":"2025-05-09T02:59:23.409630Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install segmentation-models-pytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T22:02:26.986563Z","iopub.execute_input":"2025-05-21T22:02:26.986755Z","iopub.status.idle":"2025-05-21T22:03:39.541188Z","shell.execute_reply.started":"2025-05-21T22:02:26.986730Z","shell.execute_reply":"2025-05-21T22:03:39.540264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Using pretrained unet\nimport segmentation_models_pytorch as smp\n# Define model\n# model = smp.Unet(\n#     encoder_name=\"resnet34\",        # choose encoder\n#     encoder_weights=\"imagenet\",     # use ImageNet pre-trained weights\n#     in_channels=3,                  # RGB input\n#     classes=1,                      # Binary segmentation\n# )\n# Using efficientnet\n# model = smp.Unet(\n#     encoder_name=\"efficientnet-b4\",    # EfficientNet-B4 backbone\n#     encoder_weights=\"imagenet\",        # Use ImageNet pre-trained weights\n#     in_channels=3,                     # RGB input\n#     classes=1,                         # Binary segmentation\n# )\nmodel = smp.Unet(\n    encoder_name=\"se_resnext50_32x4d\", # SE-ResNeXt50_32x4d backbone\n    encoder_weights=\"imagenet\",        # Use ImageNet pre-trained weights\n    in_channels=3,                     # RGB input\n    classes=1,                         # Binary segmentation\n)\n# efficientnet-b0\n# model = smp.Unet(\n#     encoder_name=\"efficientnet-b4\", # SE-ResNeXt50_32x4d backbone\n#     encoder_weights=\"imagenet\",        # Use ImageNet pre-trained weights\n#     in_channels=3,                     # RGB input\n#     classes=1,                         # Binary segmentation\n# )\n# model = smp.UnetPlusPlus(\n#     encoder_name=\"resnet34\",\n#     encoder_weights=\"imagenet\",\n#     in_channels=3,\n#     classes=1,\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = 'cuda'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = Unet().to(device)\nmodel = model.to(device)\n# model = ResNetUNet(n_classes=1).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Hyperparams\nfrom torch.optim import Adam\nepochs = 100\nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = Adam(model.parameters(),lr = 1e-4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:07:26.324459Z","iopub.execute_input":"2025-05-09T03:07:26.324753Z","iopub.status.idle":"2025-05-09T03:07:26.329828Z","shell.execute_reply.started":"2025-05-09T03:07:26.324732Z","shell.execute_reply":"2025-05-09T03:07:26.328985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_losses = []\nval_losses = []\nval_dice_scores = []\n\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0 \n    for images, masks in train_loader:\n        images, masks = images.to(device), masks.to(device)\n        pred = model(images)\n        loss = loss_fn(pred, masks)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        train_loss += loss.item()\n    \n    avg_train_loss = train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n    print(f\"Train loss for epoch {epoch}: {avg_train_loss:.4f}\")\n\n    model.eval()\n    val_loss = 0.0\n    dice_scores = []\n    with torch.no_grad():\n        for images, masks in valid_loader:\n            images, masks = images.to(device), masks.to(device)\n            pred = model(images)\n            loss = loss_fn(pred, masks)\n            val_loss += loss.item()\n        \n            preds = torch.sigmoid(pred)\n            preds = (preds > 0.5).float()\n            intersection = (preds * masks).sum(dim=(1,2,3))\n            union = preds.sum(dim=(1,2,3)) + masks.sum(dim=(1,2,3))\n            dice = (2. * intersection + 1e-7) / (union + 1e-7)\n            dice_scores.extend(dice.cpu().numpy())\n        \n    avg_val_loss = val_loss / len(valid_loader)\n    avg_dice = sum(dice_scores) / len(dice_scores)\n    val_losses.append(avg_val_loss)\n    val_dice_scores.append(avg_dice)\n\n    print(f\"Validation Loss: {avg_val_loss:.4f} | Dice Score: {avg_dice:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:08:53.045722Z","iopub.execute_input":"2025-05-09T03:08:53.046277Z","iopub.status.idle":"2025-05-09T03:18:38.445642Z","shell.execute_reply.started":"2025-05-09T03:08:53.046252Z","shell.execute_reply":"2025-05-09T03:18:38.444765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nepochs_range = range(epochs)\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, train_losses, label='Train Loss')\nplt.plot(epochs_range, val_losses, label='Val Loss')\nplt.legend()\nplt.title('Loss over Epochs')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, val_dice_scores, label='Val Dice Score')\nplt.legend()\nplt.title('Dice Score over Epochs')\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:18:38.446840Z","iopub.execute_input":"2025-05-09T03:18:38.447105Z","iopub.status.idle":"2025-05-09T03:18:38.878245Z","shell.execute_reply.started":"2025-05-09T03:18:38.447086Z","shell.execute_reply":"2025-05-09T03:18:38.877514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #trainingloop\n\n# for epoch in range(epochs):\n#     model.train()\n#     train_loss = 0 \n#     for images,masks in train_loader:\n#         images,masks = images.to(device),masks.to(device)\n#         pred = model(images)\n#         loss = loss_fn(pred,masks)\n#         loss.backward()\n#         optimizer.step()\n#         optimizer.zero_grad()\n#         train_loss += loss.item()\n#     print(f\"Train loss for epoch{epoch}:{train_loss/len(train_loader)}\")\n\n#     model.eval()\n#     val_loss = 0.0\n#     dice_scores = []\n#     with torch.no_grad():\n#         for images,masks in valid_loader:\n#             images,masks = images.to(device),masks.to(device)\n#             pred = model(images)\n#             loss = loss_fn(pred,masks)\n#             val_loss += loss.item()\n    \n#             preds = torch.sigmoid(pred)\n#             preds = (preds>0.5).float()\n#             # Dice score (simple implementation)\n#             intersection = (preds * masks).sum(dim=(1,2,3))\n#             union = preds.sum(dim=(1,2,3)) + masks.sum(dim=(1,2,3))\n#             dice = (2. * intersection + 1e-7) / (union + 1e-7)\n#             dice_scores.extend(dice.cpu().numpy())\n#         avg_loss = val_loss/len(valid_loader)      \n#         avg_dice = sum(dice_scores) / len(dice_scores)\n    \n#         print(f\"Validation Loss: {avg_loss:.4f} | Dice Score: {avg_dice:.4f}\")\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:04:54.498113Z","iopub.execute_input":"2025-05-09T03:04:54.498819Z","execution_failed":"2025-05-09T03:04:55.517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After training is done or when you get the best model\ntorch.save(model.state_dict(), \"unet_salt_resnet.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:22:43.012262Z","iopub.execute_input":"2025-05-09T03:22:43.013015Z","iopub.status.idle":"2025-05-09T03:22:43.168468Z","shell.execute_reply.started":"2025-05-09T03:22:43.012988Z","shell.execute_reply":"2025-05-09T03:22:43.167901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!unzip -q /kaggle/input/tgs-salt-identification-challenge/test.zip -d test_data1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:22:46.120252Z","iopub.execute_input":"2025-05-09T03:22:46.120810Z","iopub.status.idle":"2025-05-09T03:22:49.452928Z","shell.execute_reply.started":"2025-05-09T03:22:46.120774Z","shell.execute_reply":"2025-05-09T03:22:49.452089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(os.listdir('/kaggle/working/test_data1/images'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:23:12.426715Z","iopub.execute_input":"2025-05-09T03:23:12.427048Z","iopub.status.idle":"2025-05-09T03:23:12.440625Z","shell.execute_reply.started":"2025-05-09T03:23:12.427006Z","shell.execute_reply":"2025-05-09T03:23:12.440046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = Unet().to(device)\n# model.load_state_dict(torch.load('/kaggle/input/unet-resnet/pytorch/default/1/unet_salt.pth', map_location='cpu'))\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:23:13.081810Z","iopub.execute_input":"2025-05-09T03:23:13.082578Z","iopub.status.idle":"2025-05-09T03:23:13.090890Z","shell.execute_reply.started":"2025-05-09T03:23:13.082554Z","shell.execute_reply":"2025-05-09T03:23:13.090268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nimport torch\nimport torchvision.transforms as T\n\ndef rle_encode(mask):\n    '''\n    mask: numpy array, 1 - mask, 0 - background\n    Returns run length as string formatted\n    '''\n    pixels = mask.flatten(order='F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ntest_dir = '/kaggle/working/test_data1/images'\ntest_files = sorted(os.listdir(test_dir))\nresults = []\n\n# Use the same transforms as used for validation (resize, tensor, normalize)\nval_transform = T.Compose([\n    T.Resize((128, 128)),\n    T.ToTensor(),\n    T.Normalize([0.0, 0.0, 0.0], [1.0,1.0,1.0])\n     # A.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0))\n])\n\nmodel.eval()\nfor fname in tqdm(test_files):\n    img_path = os.path.join(test_dir, fname)\n    image = Image.open(img_path).convert('RGB')\n    orig_size = image.size  # (width, height)\n    input_tensor = val_transform(image).unsqueeze(0).to(device)\n    with torch.no_grad():\n        pred = model(input_tensor)\n        pred = torch.sigmoid(pred)\n        pred = (pred > 0.3).float()\n        mask = pred.squeeze().cpu().numpy().astype(np.uint8)\n    # Resize mask back to original size\n    mask = Image.fromarray(mask)\n    mask = mask.resize(orig_size, resample=Image.NEAREST)\n    mask = np.array(mask)\n    rle = rle_encode(mask)\n    img_id = os.path.splitext(fname)[0]\n    results.append({'id': img_id, 'rle_mask': rle})\n\n# Save to CSV\ndf = pd.DataFrame(results)\ndf.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T03:23:28.870799Z","iopub.execute_input":"2025-05-09T03:23:28.871480Z","iopub.status.idle":"2025-05-09T03:25:42.160360Z","shell.execute_reply.started":"2025-05-09T03:23:28.871445Z","shell.execute_reply":"2025-05-09T03:25:42.159730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T23:15:22.781017Z","iopub.execute_input":"2025-05-04T23:15:22.781368Z","iopub.status.idle":"2025-05-04T23:15:22.803397Z","shell.execute_reply.started":"2025-05-04T23:15:22.781334Z","shell.execute_reply":"2025-05-04T23:15:22.802534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}