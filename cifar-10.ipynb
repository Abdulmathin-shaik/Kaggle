{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":15444,"sourceType":"datasetVersion","datasetId":11102}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:06:16.912164Z","iopub.execute_input":"2024-12-12T05:06:16.912484Z","iopub.status.idle":"2024-12-12T05:06:16.924995Z","shell.execute_reply.started":"2024-12-12T05:06:16.912457Z","shell.execute_reply":"2024-12-12T05:06:16.923719Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cifar10-python/cifar-10-python.tar.gz\n/kaggle/input/cifar10-python/cifar-10-batches-py/data_batch_1\n/kaggle/input/cifar10-python/cifar-10-batches-py/data_batch_2\n/kaggle/input/cifar10-python/cifar-10-batches-py/batches.meta\n/kaggle/input/cifar10-python/cifar-10-batches-py/test_batch\n/kaggle/input/cifar10-python/cifar-10-batches-py/data_batch_3\n/kaggle/input/cifar10-python/cifar-10-batches-py/data_batch_5\n/kaggle/input/cifar10-python/cifar-10-batches-py/data_batch_4\n/kaggle/input/cifar10-python/cifar-10-batches-py/readme.html\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport torch\nimport torch.nn as nn\nimport cv2\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:06:16.926315Z","iopub.execute_input":"2024-12-12T05:06:16.926572Z","iopub.status.idle":"2024-12-12T05:06:16.930734Z","shell.execute_reply.started":"2024-12-12T05:06:16.926547Z","shell.execute_reply":"2024-12-12T05:06:16.929712Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"import pickle\nimport numpy as np\n\ndef unpickle(file):\n    with open(file, 'rb') as f:\n        dict = pickle.load(f, encoding='bytes')\n    return dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:06:16.931822Z","iopub.execute_input":"2024-12-12T05:06:16.932417Z","iopub.status.idle":"2024-12-12T05:06:16.938673Z","shell.execute_reply.started":"2024-12-12T05:06:16.932379Z","shell.execute_reply":"2024-12-12T05:06:16.938022Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def load_cifar10_data(data_dir):\n    train_data = []\n    train_labels = []\n\n    # Load training batches\n    for i in range(1, 6):\n        batch = unpickle(f\"{data_dir}/data_batch_{i}\")\n        train_data.append(batch[b'data'])\n        train_labels += batch[b'labels']\n\n    train_data = np.concatenate(train_data, axis=0)\n    train_data = train_data.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)  # Reshape to (N, H, W, C)\n    train_labels = np.array(train_labels)\n\n    # Load test batch\n    test_batch = unpickle(f\"{data_dir}/test_batch\")\n    test_data = test_batch[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n    test_labels = np.array(test_batch[b'labels'])\n\n    return (train_data, train_labels), (test_data, test_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:06:16.949494Z","iopub.execute_input":"2024-12-12T05:06:16.949715Z","iopub.status.idle":"2024-12-12T05:06:16.955767Z","shell.execute_reply.started":"2024-12-12T05:06:16.949694Z","shell.execute_reply":"2024-12-12T05:06:16.954828Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"\n\nclass CIFAR10Dataset(Dataset):\n    def __init__(self, data, labels, transform=None):\n        self.data = data\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image = self.data[idx]\n        label = self.labels[idx]\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n# Load data\n(data_dir) = '/kaggle/input/cifar10-python/cifar-10-batches-py'\n(train_data, train_labels), (test_data, test_labels) = load_cifar10_data(data_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:06:16.957276Z","iopub.execute_input":"2024-12-12T05:06:16.957557Z","iopub.status.idle":"2024-12-12T05:06:17.272373Z","shell.execute_reply.started":"2024-12-12T05:06:16.957525Z","shell.execute_reply":"2024-12-12T05:06:17.271350Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Define transformations\ntrain_transform = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Create PyTorch datasets and loaders\ntrain_dataset = CIFAR10Dataset(train_data, train_labels, transform=train_transform)\ntest_dataset = CIFAR10Dataset(test_data, test_labels, transform=test_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:06:17.274048Z","iopub.execute_input":"2024-12-12T05:06:17.274378Z","iopub.status.idle":"2024-12-12T05:06:17.281398Z","shell.execute_reply.started":"2024-12-12T05:06:17.274336Z","shell.execute_reply":"2024-12-12T05:06:17.280559Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:06:17.282535Z","iopub.execute_input":"2024-12-12T05:06:17.282939Z","iopub.status.idle":"2024-12-12T05:06:17.291301Z","shell.execute_reply.started":"2024-12-12T05:06:17.282911Z","shell.execute_reply":"2024-12-12T05:06:17.290475Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN,self).__init__()\n        self.conv_layer = nn.Sequential(\n            nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,stride=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.Conv2d(32,64,3,1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2),\n            nn.Conv2d(64,128,3,1),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2),\n            nn.Flatten(),\n            nn.Linear(4608,512),\n            nn.ReLU(),\n            nn.Linear(512,10)\n        )\n\n    def forward(self,x):\n        x = self.conv_layer(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:06:17.293338Z","iopub.execute_input":"2024-12-12T05:06:17.293676Z","iopub.status.idle":"2024-12-12T05:06:17.303274Z","shell.execute_reply.started":"2024-12-12T05:06:17.293646Z","shell.execute_reply":"2024-12-12T05:06:17.302332Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"model = CNN().to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:06:17.304395Z","iopub.execute_input":"2024-12-12T05:06:17.304651Z","iopub.status.idle":"2024-12-12T05:06:17.334387Z","shell.execute_reply.started":"2024-12-12T05:06:17.304626Z","shell.execute_reply":"2024-12-12T05:06:17.333758Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"import torch.optim as optim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:06:17.335167Z","iopub.execute_input":"2024-12-12T05:06:17.335403Z","iopub.status.idle":"2024-12-12T05:06:17.339202Z","shell.execute_reply.started":"2024-12-12T05:06:17.335380Z","shell.execute_reply":"2024-12-12T05:06:17.338406Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"optimizer = optim.SGD(model.parameters(),lr=1e-02)\nloss_fn = nn.CrossEntropyLoss()\nepochs = 10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:06:17.340405Z","iopub.execute_input":"2024-12-12T05:06:17.340903Z","iopub.status.idle":"2024-12-12T05:06:17.347524Z","shell.execute_reply.started":"2024-12-12T05:06:17.340876Z","shell.execute_reply":"2024-12-12T05:06:17.346694Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"train_losses,test_losses = [],[]\nfor epoch in range(epochs):\n    model.train()\n    train_loss =0\n    for batch,(x,y) in enumerate(train_loader):\n        x = x.to(device)\n        y = y.to(device)\n        pred = model(x)\n        loss = loss_fn(pred,y)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        #Metrics \n        train_loss += loss.item()\n        \n    avg_loss = train_loss/len(train_loader)\n    train_losses.append(avg_loss)\n    print(f'Epoch: {epoch} | Train loss: {avg_loss}')\n\n    model.eval()\n    with torch.no_grad():\n        test_loss,correct=0,0\n        total_samples = 0\n        for batch,(x,y) in enumerate(test_loader):\n            x = x.to(device)\n            y = y.to(device)\n            pred = model(x)\n            loss = loss_fn(pred,y)\n            total_samples += y.size(0)\n            test_loss += loss.item()\n            correct += ((torch.argmax(torch.softmax(pred,dim=1),dim=1)==y).sum().item())\n        avg_test_loss = test_loss/len(test_loader)\n        avg_correct = correct/total_samples\n        test_losses.append(avg_test_loss)\n        print(f'Test_loss {avg_test_loss} | Accuracy {avg_correct}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:06:17.348437Z","iopub.execute_input":"2024-12-12T05:06:17.348694Z","iopub.status.idle":"2024-12-12T05:06:17.997350Z","shell.execute_reply.started":"2024-12-12T05:06:17.348659Z","shell.execute_reply":"2024-12-12T05:06:17.995810Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch,(x,y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      6\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[40], line 15\u001b[0m, in \u001b[0;36mCIFAR10Dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 15\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:669\u001b[0m, in \u001b[0;36mRandomCrop.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m    img (PIL Image or Tensor): Image to be cropped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;124;03m    PIL Image or Tensor: Cropped image.\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m _, height, width \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mget_dimensions(img)\n\u001b[1;32m    672\u001b[0m \u001b[38;5;66;03m# pad the width if needed\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:526\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(img, padding, fill, padding_mode)\u001b[0m\n\u001b[1;32m    524\u001b[0m     _log_api_usage_once(pad)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mpad(img, padding\u001b[38;5;241m=\u001b[39mpadding, fill\u001b[38;5;241m=\u001b[39mfill, padding_mode\u001b[38;5;241m=\u001b[39mpadding_mode)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:152\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(img, padding, fill, padding_mode)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39munused\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpad\u001b[39m(\n\u001b[1;32m    145\u001b[0m     img: Image\u001b[38;5;241m.\u001b[39mImage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m     padding_mode: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msymmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    149\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pil_image(img):\n\u001b[0;32m--> 152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, (numbers\u001b[38;5;241m.\u001b[39mNumber, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate padding arg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mTypeError\u001b[0m: img should be PIL Image. Got <class 'numpy.ndarray'>"],"ename":"TypeError","evalue":"img should be PIL Image. Got <class 'numpy.ndarray'>","output_type":"error"}],"execution_count":47},{"cell_type":"code","source":"(torch.argmax(torch.softmax(pred,dim=1),dim=1)==y).sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:06:17.997967Z","iopub.status.idle":"2024-12-12T05:06:17.998239Z","shell.execute_reply.started":"2024-12-12T05:06:17.998106Z","shell.execute_reply":"2024-12-12T05:06:17.998119Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(train_losses,label='train')\nplt.plot(test_losses,label='test')\nplt.legend()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T05:06:17.999341Z","iopub.status.idle":"2024-12-12T05:06:17.999651Z","shell.execute_reply.started":"2024-12-12T05:06:17.999494Z","shell.execute_reply":"2024-12-12T05:06:17.999510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}